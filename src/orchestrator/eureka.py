"""Eureka-style reward generation using LLMs.

This module implements automatic reward function synthesis:
- LLM analyzes task description + environment code
- Generates executable Python reward functions
- Evolutionarily refines rewards based on training feedback

References:
- Eureka: Human-Level Reward Design (Ma et al., NVIDIA)
- Voyager: Open-Ended Embodied Agents (Wang et al.)
"""

from __future__ import annotations

import ast
import json
import os
import re
import time
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, Tuple

try:
    import httpx
except ImportError:
    httpx = None


@dataclass
class RewardCandidate:
    """A candidate reward function generated by LLM."""
    
    code: str
    description: str
    generation: int = 0
    fitness: float = 0.0
    metrics: Dict[str, float] = field(default_factory=dict)
    
    @property
    def function(self) -> Optional[Callable]:
        """Compile and return the reward function."""
        try:
            namespace: Dict[str, Any] = {}
            exec(self.code, namespace)
            return namespace.get("reward_fn")
        except Exception:
            return None


@dataclass
class EurekaConfig:
    """Configuration for Eureka reward generation."""
    
    # LLM settings
    model: str = "gpt-4"  # or "claude-3-sonnet", "llama-3-70b"
    api_base: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 1024
    
    # Evolution settings
    population_size: int = 8
    num_generations: int = 5
    mutation_rate: float = 0.3
    
    # Training settings
    eval_episodes: int = 10
    eval_max_steps: int = 1000


class EurekaRewardGenerator:
    """LLM-powered reward function synthesis.
    
    Generates reward functions from task descriptions and evolves
    them based on training feedback.
    """
    
    def __init__(self, cfg: Optional[EurekaConfig] = None):
        self.cfg = cfg or EurekaConfig()
        self.population: List[RewardCandidate] = []
        self.best_reward: Optional[RewardCandidate] = None
        self._client: Optional[Any] = None
    
    async def _init_client(self):
        if self._client is None and httpx:
            self._client = httpx.AsyncClient(timeout=60.0)
    
    async def generate_initial_population(
        self,
        task_description: str,
        env_code: str,
        obs_description: str,
    ) -> List[RewardCandidate]:
        """Generate initial population of reward functions."""
        self.population = []
        
        for i in range(self.cfg.population_size):
            candidate = await self._generate_reward(
                task_description,
                env_code,
                obs_description,
                variation_hint=f"Variation {i+1}: Try a different approach.",
            )
            self.population.append(candidate)
        
        return self.population
    
    async def _generate_reward(
        self,
        task_description: str,
        env_code: str,
        obs_description: str,
        variation_hint: str = "",
        previous_attempt: Optional[str] = None,
        previous_feedback: Optional[str] = None,
    ) -> RewardCandidate:
        """Generate a single reward function."""
        
        prompt = f"""You are an expert reward engineer for reinforcement learning.

## Task
{task_description}

## Environment Code (partial)
```python
{env_code[:2000]}
```

## Observation Space
{obs_description}

## Instructions
Write a reward function with this exact signature:
```python
def reward_fn(obs: dict, action: Any, next_obs: dict, info: dict) -> float:
    '''
    Compute reward for the given transition.
    
    Args:
        obs: Previous observation dictionary
        action: Action taken
        next_obs: New observation dictionary  
        info: Environment info dict
    
    Returns:
        float: Reward value
    '''
    # Your implementation here
    return reward
```

Requirements:
1. Use ONLY information available in obs, action, next_obs, and info
2. The reward should encourage the behavior described in the task
3. Include helpful rewards for intermediate progress, not just final goal
4. Avoid sparse rewards that make learning difficult
5. Normalize rewards to reasonable ranges (e.g., -1 to 1 for most, larger for major achievements)

{variation_hint}
"""

        if previous_attempt and previous_feedback:
            prompt += f"""

## Previous Attempt
```python
{previous_attempt}
```

## Feedback on Previous Attempt
{previous_feedback}

Please improve upon the previous attempt based on this feedback.
"""

        response = await self._query_llm(prompt)
        code = self._extract_code(response)
        
        return RewardCandidate(
            code=code,
            description=task_description,
            generation=0,
        )
    
    async def evolve(
        self,
        evaluation_fn: Callable[[RewardCandidate], Dict[str, float]],
    ) -> RewardCandidate:
        """Evolve reward functions using genetic algorithm.
        
        Args:
            evaluation_fn: Function that trains with the reward and returns metrics
        """
        for gen in range(self.cfg.num_generations):
            # Evaluate population
            for candidate in self.population:
                if candidate.fitness == 0:  # Not yet evaluated
                    metrics = evaluation_fn(candidate)
                    candidate.metrics = metrics
                    candidate.fitness = self._compute_fitness(metrics)
                    candidate.generation = gen
            
            # Sort by fitness
            self.population.sort(key=lambda c: -c.fitness)
            
            # Keep best
            self.best_reward = self.population[0]
            
            # Generate next generation
            survivors = self.population[:self.cfg.population_size // 2]
            children = []
            
            for survivor in survivors:
                # Mutate successful candidates
                if len(children) < self.cfg.population_size // 2:
                    child = await self._mutate(survivor)
                    child.generation = gen + 1
                    children.append(child)
            
            self.population = survivors + children
        
        return self.best_reward
    
    async def _mutate(self, parent: RewardCandidate) -> RewardCandidate:
        """Mutate a reward function based on training feedback."""
        feedback = self._generate_feedback(parent.metrics)
        
        return await self._generate_reward(
            task_description=parent.description,
            env_code="",  # Not needed for mutation
            obs_description="",
            previous_attempt=parent.code,
            previous_feedback=feedback,
        )
    
    def _generate_feedback(self, metrics: Dict[str, float]) -> str:
        """Generate feedback from training metrics."""
        feedback_lines = []
        
        if metrics.get("episode_return", 0) < 0:
            feedback_lines.append("The agent is receiving mostly negative rewards. Consider adding more positive intermediate rewards.")
        
        if metrics.get("reward_variance", 0) < 0.01:
            feedback_lines.append("Reward variance is very low. The reward signal may be too sparse or uninformative.")
        
        if metrics.get("success_rate", 0) < 0.1:
            feedback_lines.append("Success rate is very low. Consider adding denser reward shaping for progress towards the goal.")
        
        if not feedback_lines:
            feedback_lines.append("The reward function is working reasonably well. Try to improve efficiency or add curriculum elements.")
        
        return "\n".join(feedback_lines)
    
    def _compute_fitness(self, metrics: Dict[str, float]) -> float:
        """Compute fitness score from metrics."""
        # Weighted combination of metrics
        fitness = 0.0
        fitness += metrics.get("episode_return", 0) * 0.5
        fitness += metrics.get("success_rate", 0) * 100
        fitness -= metrics.get("reward_variance", 0) * 0.1  # Prefer stable rewards
        return fitness
    
    async def _query_llm(self, prompt: str) -> str:
        """Query LLM API."""
        await self._init_client()
        
        api_key = os.environ.get("OPENAI_API_KEY", "")
        
        if not self._client:
            # Return default reward if no client
            return """
def reward_fn(obs, action, next_obs, info):
    # Default reward: survival bonus + velocity
    reward = 0.01  # Survival
    if 'velocity' in info:
        reward += info['velocity'] * 0.001
    return reward
"""
        
        try:
            response = await self._client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "model": self.cfg.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": self.cfg.temperature,
                    "max_tokens": self.cfg.max_tokens,
                },
            )
            data = response.json()
            return data["choices"][0]["message"]["content"]
        except Exception:
            return """
def reward_fn(obs, action, next_obs, info):
    return 0.01  # Fallback
"""
    
    def _extract_code(self, response: str) -> str:
        """Extract Python code from LLM response."""
        # Look for code blocks
        code_match = re.search(r"```python\n(.*?)```", response, re.DOTALL)
        if code_match:
            return code_match.group(1).strip()
        
        # Look for function definition
        func_match = re.search(r"(def reward_fn.*?)(?=\n\n|\Z)", response, re.DOTALL)
        if func_match:
            return func_match.group(1).strip()
        
        return response


class VoyagerCurriculum:
    """Self-generating curriculum using LLM for task proposals.
    
    The agent proposes its own learning tasks based on:
    - Current capabilities (what it can already do)
    - Observation of environment (what seems possible)
    - Novelty (what it hasn't tried before)
    """
    
    def __init__(self, llm_client: Optional[Any] = None):
        self.llm = llm_client
        self.completed_tasks: List[str] = []
        self.failed_tasks: List[str] = []
        self.current_task: Optional[str] = None
        self.skill_library: Dict[str, str] = {}  # skill_name -> code
    
    async def propose_next_task(
        self,
        current_state: Dict[str, Any],
        available_skills: List[str],
    ) -> str:
        """Propose a feasible next task to expand capabilities."""
        
        prompt = f"""You are a curriculum designer for an AI game agent.

## Agent's Current State
{json.dumps(current_state, indent=2)}

## Available Skills
{', '.join(available_skills) if available_skills else 'None yet'}

## Completed Tasks
{', '.join(self.completed_tasks[-10:]) if self.completed_tasks else 'None yet'}

## Failed Tasks (avoid similar)
{', '.join(self.failed_tasks[-5:]) if self.failed_tasks else 'None'}

## Instructions
Propose ONE new task that:
1. Is achievable given current skills
2. Would expand the agent's capabilities
3. Builds towards game mastery
4. Is novel (not already completed or failed)

Output ONLY the task description (one sentence).
Example: "Navigate to the nearest shrine using the minimap"
"""

        if self.llm:
            response = await self.llm.complete(prompt)
            task = response.strip().strip('"')
        else:
            # Default curriculum without LLM
            if not self.completed_tasks:
                task = "Learn to move forward without dying"
            elif len(self.completed_tasks) < 5:
                task = "Survive for 60 seconds"
            else:
                task = "Defeat the first boss"
        
        self.current_task = task
        return task
    
    def mark_task_complete(self, task: str, skill_code: Optional[str] = None):
        """Mark a task as completed and optionally save the skill."""
        if task not in self.completed_tasks:
            self.completed_tasks.append(task)
        
        if skill_code:
            skill_name = task.replace(" ", "_").lower()[:32]
            self.skill_library[skill_name] = skill_code
        
        if self.current_task == task:
            self.current_task = None
    
    def mark_task_failed(self, task: str):
        """Mark a task as failed."""
        if task not in self.failed_tasks:
            self.failed_tasks.append(task)
        
        if self.current_task == task:
            self.current_task = None
    
    def get_progress(self) -> Dict[str, Any]:
        """Get curriculum progress summary."""
        return {
            "completed": len(self.completed_tasks),
            "failed": len(self.failed_tasks),
            "skills_learned": len(self.skill_library),
            "current_task": self.current_task,
            "recent_completions": self.completed_tasks[-5:],
        }
